{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing convolutional neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Images as data\n",
    "import matplotlib.pyplot as plt\n",
    "data = plt.imread('stop_sign.jpg')\n",
    "plt.imshow(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.shape  # (2832, 4256, 3)  Height, Width, (RGB)\n",
    "data[1000, 1500] # array([0.73333333, 0.07843137, 0.14509804])  High intensity in the red color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modifying image data\n",
    "data[:, :, 1] = 0\n",
    "data[:, :, 2] = 0\n",
    "plt.imshow(data)\n",
    "plt.show()   # Only the information in the red channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Changing an image\n",
    "data[200:1200, 200:1200, :] = [0, 1, 0]\n",
    "plt.imshow(data)\n",
    "plt.show()   # Result in an image with a green square in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Representing class data: one-hot encoding\n",
    "labels = [\"shoe\", \"dress\", \"shoe\", \"t-shirt\", \n",
    "          \"shoe\", \"t-shirt\", \"shoe\", \"dress\"]\n",
    "\n",
    "# Representing class data: one-hot encoding\n",
    "array([[0., 0., 1.],    <= shoe\n",
    "       [0., 1., 0.],    <= dress\n",
    "       [0., 0., 1.],    <= shoe\n",
    "       [1., 0., 0.],    <= t-shirt\n",
    "       [0., 0., 1.],    <= shoe\n",
    "       [1., 0., 0.],    <= t-shirt\n",
    "       [0., 0., 1.],    <= shoe\n",
    "       [0., 1., 0.]])   <= dress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row represents one sample, and each column corresponds to one of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "categories = np.array([\"t-shirt\", \"dress\", \"shoe\"])\n",
    "n_categories = 3\n",
    "ohe_labels = np.zeros((len(labels), n_categories))  # nSamples x nCategories\n",
    "for ii in range(len(labels)):\n",
    "    jj = np.where(categories == labels[ii])\n",
    "    ohe_labels[ii, jj] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python3\n",
    "test\n",
    "\n",
    "array([[0., 0., 1.], \n",
    "       [0., 1., 0.], \n",
    "       [0., 0., 1.], \n",
    "       [0., 1., 0.], \n",
    "       [0., 0., 1.],\n",
    "       [0., 0., 1.],\n",
    "       [0., 0., 1.],\n",
    "       [0., 1., 0.]])\n",
    "\n",
    "prediction\n",
    "\n",
    "array([[0., 0., 1.], \n",
    "       [0., 1., 0.], \n",
    "       [0., 0., 1.], \n",
    "       [1., 0., 0.], <= incorrect\n",
    "       [0., 0., 1.],\n",
    "       [1., 0., 0.],\n",
    "       [0., 0., 1.], <= incorrect\n",
    "       [0., 1., 0.]])\n",
    "       \n",
    "(test * prediction).sum()\n",
    "6.0       # The number of correct classifications.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keras for image classification\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "\n",
    "from keras.layers import Dense\n",
    "train_data.shape  # (50, 28, 28, 1)  50 samples, each sample is 28x28 size black and white image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fully connected layers.\n",
    "model.add(Dense(10, activation='relu', input_shape=(784,)))  # Num of nodes -> model complexity\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax')) # Output units. softmax for multiclasses classification problem.\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "train_data = train_data.reshape((50, 784))\n",
    "\n",
    "model.fit(train_data, train_labels, \n",
    "          validation_split=0.2,\n",
    "          epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "Train on 40 samples, validate on 10 samples\n",
    "Epoch 1/3\n",
    "\n",
    "32/40 [=======================>......] - ETA: 0s - loss: 1.0117 - acc: 0.4688\n",
    "40/40 [==============================] - 0s 4ms/step - loss: 1.0438 - acc: 0.4250 - val_loss: 0.9668 - val_acc: 0.4000\n",
    "Epoch 2/3\n",
    "\n",
    "32/40 [=======================>......] - ETA: 0s - loss: 0.9556 - acc: 0.5312\n",
    "40/40 [==============================] - 0s 195us/step - loss: 0.9404 - acc: 0.5750 - val_loss: 0.9068 - val_acc: 0.4000\n",
    "Epoch 3/3\n",
    "\n",
    "32/40 [=======================>......] - ETA: 0s - loss: 0.9143 - acc: 0.5938\n",
    "40/40 [==============================] - 0s 189us/step - loss: 0.8726 - acc: 0.6750 - val_loss: 0.8452 - val_acc: 0.4000\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = test_data.reshape((10, 784))\n",
    "model.evaluate(test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "10/10 [==============================] - 0s 335us/step\n",
    "[1.0191701650619507, 0.4000000059604645]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using correlations in images\n",
    "- Natural images contain spatial correlations\n",
    "- For example, pixels along a contour or edge\n",
    "- How can we use these correlations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological inspiration\n",
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/c7810cb749aa1003f9fbb0ceead35c2ccda197d2/Visual_map_Swindale_Monkey_ori_domains_Blasdel_1986.jpg\" width=\"400\">  \n",
    "  \n",
    "Our own visual system uses these correlations, and each nerve cell in the visual areas in our brain responds to oriented edges at a particular location inthe visual field.  \n",
    "This image depicts a small part of the visual cortex(the scale bar is 1mm in size).  \n",
    "Each part of the image responds to some part of the visual field, and to the orientation depicted by the colors on the right.  \n",
    "  \n",
    "Looking for the same feature, such as a particular orientation, in every location in an image is like a mathematical operation called a convolution.  \n",
    "This is the fundamental operation that convolutional neural networks use to preccess images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a convolution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array contains an \"edge\" in the middle, where the values go from zero to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel = np.array([-1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel defines the feature that we are looking for.  \n",
    "In this case, we are looking for a change from small values on the left to large values on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv[0] = (kernel * array[0:2]).sum()\n",
    "conv[1] = (kernel * array[1:3]).sum()\n",
    "conv[2] = (kernel * array[2:4]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the result as all zeros.  \n",
    "Then, we slide the kernel along the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ii in range(8):\n",
    "    conv[ii] = (kernel * array[ii:ii+2]).sum()\n",
    "conv # array([0, 0, 0, 0, 1, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each location we multiply the values in the array with the values in the kernel and sum them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array = np.array([0, 0, 1, 1, 0, 0, 1, 1, 0, 0])\n",
    "kernel = np.array([-1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the array goes between 0 and 1 twice.  \n",
    "In this case, the edges that go from zero to one match the kernel, but the edges from 1 to 0 are the opposite of the kernel.  \n",
    "In these locations, the convolution becomes negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "for ii in range(8):\n",
    "    conv[ii] = (kernel * array[ii:ii+2]).sum()\n",
    "conv # array([ 0,  1,  0, -1,  0,  1,  0, -1,  0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image convolution\n",
    "Convolutions of images do the same operation, but in two dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-dimensional convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel = np.array([[-1, 1], \n",
    "                   [-1, 1]])\n",
    "conv = np.zeros((27, 27)\n",
    "for ii in range(27):\n",
    "    for jj in range(27):\n",
    "        window = image[ii:ii+2, jj:jj+2]    # with same size of kernel.\n",
    "        conv[ii, jj] = np.sum(window * kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/6c00959d6fb39d4e14fb369cb605a6aa21562e75/no_padding_no_strides.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **kernel** is the gray 3-by-3 box that slides over the blue input image at the bottom.  \n",
    "In each location, the window is multiplied with the values in the kernel and added up to create the value of one of the pixels in the resulting green array at the top.  \n",
    "In neural networks, we call this resulting array a **\"Feature map\"**, because it contains a map of the locations in the image that match the feature represented by this kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image convolutions\n",
    "The convolution of an image with a kernel summarizes a part of the image as the sum of the multiplication of that part of the image with the kernel.  \n",
    "In this exercise, you will write the code that executes a convolution of an image with a kernel using Numpy.  \n",
    "Given a black and white image that is stored in the variable `im`,  \n",
    "write the operations inside the loop that would execute the convolution with the provided kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the right window from the image in each iteration and multiply this part of the image with the kernel.  \n",
    "Sum the result and allocate the sum to the correct entry in the output array `(results).`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel = np.array([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\n",
    "result = np.zeros(im.shape)\n",
    "\n",
    "# Output array\n",
    "for ii in range(im.shape[0] - 3):\n",
    "    for jj in range(im.shape[1] - 3):\n",
    "        result[ii, jj] = (im[ii:ii+3, jj:jj+3] * kernel).sum()\n",
    "\n",
    "# Print result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[2.68104586 2.95947725 2.84313735 ... 0.         0.         0.        ]\n",
    " [3.01830077 3.07058835 3.05098051 ... 0.         0.         0.        ]\n",
    " [2.95163405 3.09934652 3.20261449 ... 0.         0.         0.        ]\n",
    " ...\n",
    " [0.         0.         0.         ... 0.         0.         0.        ]\n",
    " [0.         0.         0.         ... 0.         0.         0.        ]\n",
    " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining image convolution kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be asked to define the kernel that finds a particular feature in the image.  \n",
    "  \n",
    "For example, the following kernel finds a vertical line in images:  \n",
    "```  \n",
    "np.array([[-1, 1, -1], \n",
    "          [-1, 1, -1], \n",
    "          [-1, 1, -1]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define a kernel that finds horizontal lines in images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel = np.array([[-1, -1, -1], \n",
    "                   [1, 1, 1],\n",
    "                   [-1, -1, -1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Define a kernel that finds a light spot surrounded by dark pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel = np.array([[-1, -1, -1], \n",
    "                   [-1, 1, -1],\n",
    "                   [-1, -1, -1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Define a kernel that finds a dark spot surrounded by bright pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel = np.array([[1, 1, 1], \n",
    "                   [1, -1, 1],\n",
    "                   [1, 1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing image convolutions in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras 'Convolution' layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "Conv2D(10, kernel_size=3, activation='relu')   # 10 convolution units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It resembles the \"Dense\" layers, but instead of having every unit in the layer connected to every unit in the previous layer,  \n",
    "these connect to the previous layer through a convolution kernel.  \n",
    "This means that the output of each unit in this layer is a convolution of a kernel over the image input.    \n",
    "During training of a neural network that has convolutional layers, the kernels in each unit would be adjusted using back-propagation.  \n",
    "  \n",
    "A dense layer has one weight for each pixel in the image, but a convolution layer has only one weight for each pixel in the kernel.  \n",
    "For example, if we set the kernel size argument to 3, that means that the kernel of each unit has 9 pixels.  \n",
    "If the layer has 10 units, it would have 90 parameters for these kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating convolution layers into a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "model = Sequential()\n",
    "model.add(Conv2D(10, kernel_size=3, activation='relu', \n",
    "              input_shape=(img_rows, img_cols, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten layer serves as a connector between convolution and densely connected layers.  \n",
    "This takes the output of the convolution that we previously referred to as a \"feature map\",   \n",
    "and flattens it into a **one-dimensional array.** &rarr; the expected input into the densely connected layer.   \n",
    "  \n",
    "Here, the output is one of three classes of clothing, so there are three units.  \n",
    "To classify among the categories represented by the three units, we use the softmax activation function. \n",
    "  \n",
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/4acb90dffc2d4226bf351ba01e164b60e3188541/conv2d_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification tasks categorical cross-entropy is an appropriate loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.shape # (50, 28, 28, 1)  50 training items, each has 28 by 28 pixel image with one channel.\n",
    "model.fit(train_data, train_labels, validation_split=0.2, epochs=3)\n",
    "model.evaluate(test_data, test_labels, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional network for image classification\n",
    "Convolutional networks for classification are constructed from a sequence of convolutional layers (for image processing) and fully connected (Dense) layers (for readout).  \n",
    "In this exercise, you will construct a small convolutional network for classification of the data from the fashion dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "Add a Conv2D layer to construct the input layer of the network. Use a kernel size of 3 by 3.  \n",
    "You can use the img_rows and img_cols objects available in your workspace to define the input_shape of this layer.  \n",
    "Add a Flatten layer to translate between the image processing and classification part of your network.  \n",
    "Add a Dense layer to classify the 3 different categories of clothing in the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the necessary components from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "# Initialize the model object\n",
    "model = Sequential()\n",
    "\n",
    "# Add a convolutional layer\n",
    "model.add(Conv2D(10, kernel_size=3, activation='relu', \n",
    "               input_shape=(img_rows,img_cols,1)))\n",
    "\n",
    "# Flatten the output of the convolutional layer\n",
    "model.add(Flatten())\n",
    "# Add an output layer for the 3 categories\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a CNN to classify clothing types\n",
    "Before training a neural network it needs to be compiled with the right cost function, using the right optimizer.  \n",
    "During compilation, you can also define metrics that the network calculates and reports in every epoch.  \n",
    "Model fitting requires a training data set, together with the training labels to the network.  \n",
    "  \n",
    "The Conv2D model you built in the previous exercise is available in your workspace.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "Compile the network using the 'adam' optimizer and the 'categorical_crossentropy' cost function.  \n",
    "In the metrics list define that the network to report 'accuracy'.  \n",
    "Fit the network on train_data and train_labels. Train for 3 epochs with a batch size of 10 images.  \n",
    "In training, set aside 20% of the data as a validation set, using the validation_split keyword argument.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the model \n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit the model on a training set\n",
    "model.fit(train_data, train_labels, \n",
    "          validation_split=0.2, \n",
    "          epochs=3, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a CNN with test data\n",
    "To evaluate a trained neural network, you should provide a separate testing data set of labeled images.  \n",
    "The model you fit in the previous exercise is available in your workspace.  \n",
    "#### Instructions\n",
    "Evaluate the data on a separate test set: test_data and test_labels.  \n",
    "Use the same batch size that was used for fitting (10 images per batch).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model on separate test data\n",
    "model.evaluate(test_data, test_labels, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[0.5284238457679749, 1.0]\n",
    "```\n",
    "The first number in the output is the value of the cross-entropy loss, the second is the value of the accuracy. For this model, it's 100%!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweaking your convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/6c00959d6fb39d4e14fb369cb605a6aa21562e75/no_padding_no_strides.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue input image is larger than the green output image because the convolution kernel has the size of 3 by 3 pixels.  \n",
    "In this case, it converts a 3 by 3 window into one pixel in the output image.  \n",
    "One way to deal with this issue is to **zero-pad** the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/d372dfbd801c83213bd137655da896e28eb7b5b3/convolution_animation.gif\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dashed boxes around the central image are zeros that are added to the image.  \n",
    "When the input image is zero padded, the output feature map has the same size as the input.  \n",
    "This can be useful if you want to build a network that has many layers.  \n",
    "Otherwise, you might lose a pixel off the edge of the image in each subsequent layer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero padding in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(10, kernel_size=3, activation='relu', \n",
    "                 input_shape=(img_rows, img_cols, 1)),\n",
    "                 padding='valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement zero padding in keras, we will use the Conv2D object's padding keyword argument.    \n",
    "If we provide the value **'valid'**, **no zero padding** is added(default)  \n",
    "On the other hand, if we provide the value **'same'**, **zero padding will be applied** to the input to this layer,  \n",
    "so that the output of the convolution has the same size as the input into the convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(10, kernel_size=3, activation='relu', \n",
    "                 input_shape=(img_rows, img_cols, 1)),\n",
    "                 padding='same')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another factor that affects the size of the output of a convolution is the size of the step that we take with the kernel between input pixels.  \n",
    "This is called the size of the **stride**.\n",
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/25bb19212056fe26e3f73e246a323768c1881fbf/padding_strides.gif\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this animation the kernel is trided by two pixels in each step. This means again that the output size is smaller than the input size.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strides in Keras\n",
    "Strides are also implemented as a keyword argument to the Conv2D layers.  \n",
    "The default is for the stride to be set to 1.  \n",
    "If the stride is set to more than 1, the kernel jumps in steps of that number of pixels.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(10, kernel_size=3, activation='relu', \n",
    "                 input_shape=(img_rows, img_cols, 1)),\n",
    "                 strides=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the size of the output\n",
    "$ O\\ =\\ ((I\\ -\\ K\\ +\\ 2P)\\ /\\ S)\\ +\\ 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where   \n",
    "  \n",
    "- $O$ = size of the output\n",
    "- $I$ = size of the input\n",
    "- $K$ = size of the kernel\n",
    "- $P$ = size of the zero padding\n",
    "- $S$ = strides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dilated convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/228817e1e9e70f9d2afdba84d99cc6cf48648990/dilation.gif\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can also tweak the spacing between the pixels affected by the kernel.  \n",
    "In this case, the convolution kernel has only 9 parameters, but it has the same field of view as a kernel that would have the size 5 by 5.  \n",
    "This is **useful** in cases where you **need to aggregate information across multiple scales**.  \n",
    "This too is controlled through a keyword argument **'dilation_rate'**, that sets the distance between subsequent pixels.  \n",
    "#### Dilation in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(10, kernel_size=3, activation='relu', \n",
    "                 input_shape=(img_rows, img_cols, 1)),\n",
    "                 dilation_rate=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add padding to a CNN\n",
    "Padding allows a convolutional layer to retain the resolution of the input into this layer.  \n",
    "This is done by adding zeros around the edges of the input image, so that the convolution kernel can overlap with the pixels on the edge of the image.  \n",
    "  \n",
    "#### Instructions\n",
    "Add a Conv2D layer and choose a padding such that the output has the same size as the input.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the convolutional layer\n",
    "model.add(Conv2D(10, kernel_size=3, activation='relu', \n",
    "                 input_shape=(img_rows, img_cols, 1), \n",
    "                 padding='same'))\n",
    "\n",
    "# Feed into output layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add strides to a convolutional network\n",
    "The size of the strides of the convolution kernel determines whether the kernel will skip over some of the pixels as it slides along the image.  \n",
    "This affects the size of the output because when strides are larger than one, the kernel will be centered on only some of the pixels.  \n",
    "#### Instructions\n",
    "Construct a neural network with a Conv2D layer with strided convolutions that skips every other pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add the convolutional layer\n",
    "model.add(Conv2D(10, kernel_size=3, activation='relu', \n",
    "              input_shape=(img_rows, img_cols, 1), \n",
    "              strides=2))\n",
    "\n",
    "# Feed into output layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the size of convolutional layer output\n",
    "Zero padding and strides affect the size of the output of a convolution.  \n",
    "What is the size of the output for an input of size 256 by 256, with a kernel of size 4 by 4, padding of 1 and strides of 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ O\\ =\\ ((I\\ -\\ K\\ +\\ 2P)\\ /\\ S)\\ +\\ 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$O\\ =\\ ((256\\ -\\ 4\\ +\\ 2\\ \\dot\\ 1)\\ /\\ 2)\\ +\\ 1\\ =\\ 128 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the major strengths of convolutional neural networks comes from building networks with **multiple layers** of convolutional filters.  \n",
    "This is why using artificial neural networks is sometimes also called **\"Deep Learning\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/4acb90dffc2d4226bf351ba01e164b60e3188541/conv2d_1.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has one convolutional layer, followed by a flattening and readout with a fully connected layer with 3 units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation of this network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(10, kernel_size=2, activation='relu', \n",
    "                 input_shape=(img_rows, img_cols, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deeper network\n",
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/cdc8c93bd1986368c7e9bad5a0258f8d1d513935/Conv2D_2.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(10, kernel_size=2, activation='relu', \n",
    "                 input_shape=(img_rows, img_cols, 1), \n",
    "                 padding='equal'))\n",
    "# Second convolutional layer\n",
    "model.add(Conv2D(10, kernel_size=2, activation='relu')\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we want deep networks?\n",
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/89a36121b1a50776992a9447b499efa564da411d/googlenet.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is again motivated by our own visual system, which has multiple layers of processing in it.   \n",
    "For example, this is the architecture of a network developed by Google researchers in 2014.  \n",
    "It has 22 layers of convolutions, and some other kinds of layers, like pooling layers.  \n",
    "For the time being, one way to understand why we would want a network this deep is by looking at the kinds of things that the kernels and feature maps in the different layers tend to respond to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features in early layers\n",
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/3502a5aeb3de0cd032447b616583171cf65c9a12/layer2_viz.png\" width=\"200\">\n",
    "\n",
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/264ef75cf9c131c20ff627ffc69b1941dbff24fd/layer2_2_viz.png\" width=\"200\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the kinds of things that layers in the early part of the network tend to respond to.  \n",
    "**Oriented lines**, or **simple textures**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features in intermediate layers\n",
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/20dcdf073a4ee899d3aeaf00f1594e42c1547bb4/layer4a_viz.png\" width=\"200\">\n",
    "\n",
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/8457337f9d46a7e64f8b15398cc11eae0954e090/layer4b_viz.png\" width=\"200\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intermediate layers of the network tend to respond to **more complex features**, that include **simple objects**, such as eyes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features in late layers\n",
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/94c904a51c33d5a64cb806b0c56c905cd2bd4a03/layer4e_viz.png\" width=\"200\">\n",
    "\n",
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/05b02ebadec6bb485f1d08b1a3f3673ac373aec1/layer4d_viz.png\" width=\"200\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the time the information travels up to higher layers of the network, the feature maps tend to extract **specific types of objects**.  \n",
    "This aloows the fully connected layers at the top of the network to extract useful information for object classification based on the responses of these layers.  \n",
    "In other words, **having multiple layers of convolutions in the network** allows the network to **gradually build up representations of objects in the images from simple features to more complex features** and up to **sensitivity to distinct categories of objects**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How deep?\n",
    "- Depth comes at a computational cost\n",
    "- May require more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "### Creating a deep learning network\n",
    "A deep convolutional neural network is a network that has more than one layer.  \n",
    "Each layer in a deep network receives its input from the preceding layer,  \n",
    "with the very first layer receiving its input from the images used as training or test data.  \n",
    "  \n",
    "Here, you will create a network that has two convolutional layers.  \n",
    "  \n",
    "#### Instructions\n",
    "The first convolutional layer is the input layer of the network. This should have 15 units with kernels of 2 by 2 pixels.  \n",
    "It should have a 'relu' activation function. It can use the variables img_rows and img_cols to define its input_shape.  \n",
    "The second convolutional layer receives its inputs from the first layer. It should have 5 units with kernels of 2 by 2 pixels.  \n",
    "It should also have a 'relu' activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Add a convolutional layer (15 units)\n",
    "model.add(Conv2D(15, kernel_size=2, activation='relu', input_shape=(img_rows, img_cols, 1)))\n",
    "\n",
    "\n",
    "# Add another convolutional layer (5 units)\n",
    "model.add(Conv2D(5, kernel_size=2, activation='relu'))\n",
    "\n",
    "# Flatten and feed to output layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a deep CNN to classify clothing images\n",
    "Training a deep learning model is very similar to training a single layer network. Once the model is constructed (as you have done in the previous exercise), the model needs to be compiled with the right set of parameters.  \n",
    "Then, the model is fit by providing it with training data, as well as training labels.  \n",
    "After training is done, the model can be evaluated on test data.  \n",
    "  \n",
    "The model you built in the previous exercise is available in your workspace.  \n",
    "   \n",
    "#### Instructions\n",
    "Compile the model to use the categorical cross-entropy loss function and the Adam optimizer.  \n",
    "Train the network with train_data for 3 epochs with batches of 10 images each.  \n",
    "Use randomly selected 20% of the training data as validation data during training.  \n",
    "Evaluate the model with test_data, use a batch size of 10.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to training data \n",
    "model.fit(train_data, train_labels, \n",
    "          validation_split=0.2, \n",
    "          epochs=3, batch_size=10)\n",
    "\n",
    "# Evaluate the model on test data\n",
    "model.evaluate(test_data, test_labels, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Train on 40 samples, validate on 10 samples\n",
    "    Epoch 1/3\n",
    "    \n",
    "    10/40 [======>.......................] - ETA: 0s - loss: 1.0957 - acc: 0.4000\n",
    "    40/40 [==============================] - 0s 6ms/step - loss: 1.0861 - acc: 0.5750 - val_loss: 1.0628 - val_acc: 0.7000\n",
    "    Epoch 2/3\n",
    "    \n",
    "    10/40 [======>.......................] - ETA: 0s - loss: 1.0664 - acc: 0.7000\n",
    "    40/40 [==============================] - 0s 927us/step - loss: 1.0376 - acc: 0.8750 - val_loss: 0.9896 - val_acc: 1.0000\n",
    "    Epoch 3/3\n",
    "    \n",
    "    10/40 [======>.......................] - ETA: 0s - loss: 1.0019 - acc: 1.0000\n",
    "    40/40 [==============================] - 0s 935us/step - loss: 0.9547 - acc: 0.9750 - val_loss: 0.8810 - val_acc: 1.0000\n",
    "    \n",
    "    10/10 [==============================] - 0s 479us/step\n",
    "\n",
    "Accuracy calculated on the test data is not subject to overfitting.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When considering the architecture of networks, it is sometimes useful to think about the number of parameters in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(10, activation='relu', \n",
    "          input_shape=(784,)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first layer has 10 units. Each one of them is connected to each one of the pixels in the image throught a weight.  \n",
    "The second layer has 10 units, and each one of these is connected to all the units in the first layer.  \n",
    "Finally, each one of the units in the last layer is connected to each of the units in layer two.  \n",
    "  \n",
    "When you construct a Keras model, you can get a description of this model by calling the model's summary method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call the summary method \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_1 (Dense)              (None, 10)                7850      \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 10)                110       \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 3)                 33        \n",
    "=================================================================\n",
    "Total params: 7,993\n",
    "Trainable params: 7,993\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the total number of parameters in the model is 7,993."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. First layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(10, \n",
    "                activation='relu', \n",
    "                input_shape=(784,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$parameters\\ =\\ 784\\ \\dot\\ 10\\ +\\ 10\\ =\\ 7850$  \n",
    "**Every pixel** in the image, 784 pixels, **times** **the number of units in this layer**, 10 units **+** 10 parameters for **bias** terms in every one of these units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(10, \n",
    "                activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$parameters\\ =\\ 10\\ \\dot\\ 10\\ +\\ 10\\ =\\ =\\ 110$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(3,\n",
    "               activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$parameters\\ =\\ 10\\ \\dot\\ 3\\ +\\ 3\\ =\\ 33$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of parameters is 7850 + 110 + 33 = 7993"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The number of parameters in a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(10, kernel_size=3, activation='relu', \n",
    "                 input_shape=(28, 28, 1), padding='same'))\n",
    "model.add(Conv2D(10, kernel_size=3, activation='relu', \n",
    "                 padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_1 (Conv2D)           (None, 28, 28, 10)        100       \n",
    "_________________________________________________________________\n",
    "conv2d_2 (Conv2D)           (None, 28, 28, 10)        910       \n",
    "_________________________________________________________________\n",
    "flatten_3 (Flatten)          (None, 7840)              0         \n",
    "_________________________________________________________________\n",
    "dense_4 (Dense)              (None, 3)                 23523     \n",
    "=================================================================\n",
    "Total params: 24,533\n",
    "Trainable params: 24,533\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. First layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(10, kernel_size=3, \n",
    "          activation='relu', \n",
    "          input_shape=(28, 28, 1), \n",
    "          padding='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$parameters\\ =\\ 9\\ \\dot\\ 10\\ +\\ 10\\ =\\ 100$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 kernels with 9 parameters each, + 10 bias terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(10, kernel_size=3, \n",
    "                 activation='relu', \n",
    "                 padding='same'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$parameters\\ =\\ 10\\ \\dot\\ 9\\ \\dot\\ 10\\ +\\ 10\\ =\\ 910$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each unit is connected through a convolutional kernel to each feature map in the first layer.  \n",
    "That's 10 times 9 times 10 parameters, and a bias term for each unit, which is a total of 910."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Flatten layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **flatten layer has no parameters at all**.  \n",
    "It just takes the output from the feature maps in layer 2 and flattens them into one big array.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(3, \n",
    "                activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$parameters\\ =\\ 7840\\ \\dot\\ 3\\ +\\ 3\\ =\\ 23523$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there is zero padding here, the convolutions leaves the same number of pixels in each subsequent layer, so we end up with 28 by 28 pixels in each feature map, with 10 feature maps or 7840 pixels in total times 3 units in the last layer is 23520 + 3 bias terms is 23523."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding together, we get 24533.  \n",
    "So **convolutional layers don't necessarily reduce the number of parameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그냥 units이 증가하는 Dense layer에서는 첫번째 layer에 parameter가 몰려있다.  \n",
    "하지만 CNN에서는 Conve layer의 parameter는 별로 없고 마지막 Dense layer에 parameter가 몰려있다.  \n",
    "  \n",
    "One way to think about this is that the convolutions have more expressive power, so they require less parameters,  \n",
    "but reading out these more expressive representations then requires many more parameters on the output side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the challenges in fitting neural networks inis the large number of parameters.  \n",
    "One way to mitigate this is to summarize the output of convolutional layers in concise manner.  \n",
    "To do this, we can use pooling operations.   \n",
    "  \n",
    "For example, we might summarize a group of pixels based on its maximal value. This is called \"max pooling\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/44bb48885e3c2c92a2defc3ef9695e74134e9e7c/maxpooling6.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/02ff70485a5583941a0699352430ad9ebbaa7d66/maxpooling_result.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace pixels with one large pixel that stores its maximal value.  \n",
    "If we repeat this operation in multiple windows of size 2 by 2, we end up with an image that has a quarter of the number of the original pixels, and retains only the brightest feature in each part of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = np.zeros((im.shape[0]//2, im.shape[1]//2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by allocating the output. This has half as many pixels on each dimension as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result[0, 0] = np.max(im[0:2, 0:2])\n",
    "result[0, 1] = np.max(im[0:2, 2:4])\n",
    "result[0, 2] = np.max(im[0:2, 4:6])\n",
    "\n",
    "# ...\n",
    "\n",
    "result[1, 0] = np.max(im[2:4, 0:2])\n",
    "result[1, 1] = np.max(im[2:4, 2:4])\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start from the first coordinate in the output, calculating the maximum of the image in the first two coordinates on each dimension of the input.  \n",
    "Next, we slide along the window by 2 pixels along the first dimension, calculating the maximum for this window.  \n",
    "We keep going like that, until we are done with the first row in the input.  \n",
    "  \n",
    "We then move the window to the beginning of the second row in the input, calculating the maximum for coordinates in the third and fourth rows in the input for this location.  \n",
    "We continue sliding the window along.  \n",
    "Ultimately, in each location in the output, we calculate the maximum for a window of 2 by 2 pixels at the corresponding location in the input.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another way of implementing this operation is with a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for ii in range(result.shape[0]):\n",
    "    for jj in range(result.shape[1]):\n",
    "        result[ii, jj] = np.max(im[ii*2:ii*2+2, jj*2:jj*2+2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each iteration, we first select the corresponding rows: from the current row in the output, index ii, times two, and until 2 pixels beyond that.  \n",
    "And the same for the internal loop on the column index jj.  \n",
    "This performs the same operation that we previously broke down row by row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max pooling in Keras\n",
    "We can integrate max pooling operations into a Keras convolutional neural network, using the **MaxPool2D object**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, MaxPool2D\n",
    "model = Sequential()\n",
    "model.add(Conv2D(5, kernel_size=3, activation='relu', \n",
    "              input_shape=(img_rows, img_cols, 1)))\n",
    "model.add(MaxPool2D(2))\n",
    "model.add(Conv2D(15, kernel_size=3, activation='relu', \n",
    "              input_shape=(img_rows, img_cols, 1)))\n",
    "model.add(MaxPool2D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each convolutional layer, we'll add a pooling layer.  \n",
    "The input to the MaxPool2D object, two in this case, is the size of the pooling window.  \n",
    "That means that here pooling will take the max over a window of two by two pixels from the input for each location in the output.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_1 (Conv2D)            (None, 26, 26, 5)         50        \n",
    "_________________________________________________________________\n",
    "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 5)         0         \n",
    "_________________________________________________________________\n",
    "conv2d_2 (Conv2D)            (None, 11, 11, 15)        690       \n",
    "_________________________________________________________________\n",
    "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 15)          0         \n",
    "_________________________________________________________________\n",
    "flatten_1 (Flatten)          (None, 375)               0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 1128      \n",
    "=================================================================\n",
    "Total params: 1,868\n",
    "Trainable params: 1,868\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that using the pooling operatino dramatically reduces the number of parameters in the model.  \n",
    "Instead of more than 30,000 parameters that this model had with no pooling operations, we now have less than 2,000 parameters.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write your own pooling operation\n",
    "As we have seen before, CNNs can have a lot of parameters.  \n",
    "Pooling layers are often added between the convolutional layers of a neural network to summarize their outputs in a condensed manner, and reduce the number of parameters in the next layer in the network.  \n",
    "This can help us if we want to train the network more rapidly, or if we don't have enough data to learn a very large number of parameters.  \n",
    "  \n",
    "A pooling layer can be described as a particular kind of convolution.   \n",
    "For every window in the input it finds the maximal pixel value and passes only this pixel through.   \n",
    "In this exercise, you will write your own max pooling operation, based on the code that you previously used to write a two-dimensional convolution operation.  \n",
    "  \n",
    "#### Instructions\n",
    "Index into the input array (im) and select the right window.  \n",
    "Find the maximum in this window.  \n",
    "Allocate this into the right entry in the output array (result).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Result placeholder\n",
    "result = np.zeros((im.shape[0]//2, im.shape[1]//2))\n",
    "\n",
    "# Pooling operation\n",
    "for ii in range(result.shape[0]):\n",
    "    for jj in range(result.shape[1]):\n",
    "        result[ii, jj] = np.max(im[ii*2:ii*2+2, jj*2:jj*2+2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting image is **smaller**, but **retains the salient features** in every location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras pooling layers\n",
    "Keras implements a pooling operation as a layer that can be added to CNNs between other layers.  \n",
    "In this exercise, you will construct a convolutional neural network similar to the one you have constructed before:  \n",
    "  \n",
    "Convolution => Convolution => Flatten => Dense  \n",
    "  \n",
    "However, you will also add a pooling layer.  \n",
    "The architecture will add a single max-pooling layer between the convolutional layer and the dense layer with a pooling of 2x2:  \n",
    "  \n",
    "Convolution => Max pooling => Convolution => Flatten => Dense  \n",
    "  \n",
    "A Sequential model along with Dense, Conv2D, Flatten, and MaxPool2D objects are available in your workspace.  \n",
    "  \n",
    "#### Instructions\n",
    "Add an input convolutional layer (15 units, kernel size of 2, relu activation).  \n",
    "Add a maximum pooling operation (pooling over windows of size 2x2).  \n",
    "Add another convolution layer (5 units, kernel size of 2, relu activation).  \n",
    "Flatten the output of the second convolution and add a Dense layer for output (3 categories, softmax activation).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add a convolutional layer\n",
    "model.add(Conv2D(15, kernel_size=2, activation='relu', \n",
    "                 input_shape=(img_rows, img_cols, 1)))\n",
    "\n",
    "# Add a pooling operation\n",
    "model.add(MaxPool2D(2))\n",
    "\n",
    "# Add another convolutional layer\n",
    "model.add(Conv2D(5, kernel_size=2, activation='relu'))\n",
    "\n",
    "# Flatten and feed to output layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "conv2d_1 (Conv2D)            (None, 27, 27, 15)        75        \n",
    "_________________________________________________________________\n",
    "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 15)        0         \n",
    "_________________________________________________________________\n",
    "conv2d_2 (Conv2D)            (None, 12, 12, 5)         305       \n",
    "_________________________________________________________________\n",
    "flatten_1 (Flatten)          (None, 720)               0         \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 3)                 2163      \n",
    "=================================================================\n",
    "Total params: 2,543\n",
    "Trainable params: 2,543\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is even deeper, but has fewer parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a deep CNN with pooling to classify images\n",
    "Training a CNN with pooling layers is very similar to training of the deep networks that y have seen before.  \n",
    "Once the network is constructed (as you did in the previous exercise), the model needs to be appropriately compiled, and then training data needs to be provided, together with the other arguments that control the fitting procedure.  \n",
    "  \n",
    "The following model from the previous exercise is available in your workspace:  \n",
    "  \n",
    "Convolution => Max pooling => Convolution => Flatten => Dense  \n",
    "  \n",
    "#### Instructions\n",
    "Compile this model to use the categorical cross-entropy loss function and the Adam optimizer.  \n",
    "Train the model for 3 epochs with batches of size 10.  \n",
    "Use 20% of the data as validation data.  \n",
    "Evaluate the model on test_data with test_labels (also batches of size 10).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit to training data\n",
    "model.fit(train_data, train_labels, epochs=3, batch_size=10, validation_split=0.2)\n",
    "\n",
    "# Evaluate on test data \n",
    "model.evaluate(test_data, test_labels, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During learning, the weights used by the network change, and as they change, the network becomes more attuned to the features of the images that discriinate between calsses.  \n",
    "This means that the loss function we use for training becomes smaller and smaller.  \n",
    "Looking at the change in the loss with learning can be helpful to see whether learning is progressing as expected, and whether the network has learned enough.  \n",
    "As long as learning is progressing well, we might expect the loss function to keep going down.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch마다 Loss function을 그려보면 Training set에대해선 계속 내려가는데, Validation set에 대해서는 어느순간 다시 계속 증가하는 곳이 존재.  \n",
    "뉴럴 네트워크가 매우 많은 파라미터들을 갖고 있기 때문에 학습을 계속할수록 트레이닝 데이터에 대해서는 정확하게 맞춰갈 수 있다.  \n",
    "하지만 트레이닝 데이터만 맞춰지고 일반화 되지 않는다! **Overfitting**  \n",
    "Validation loss가 증가하기 전에 학습을 마치자!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training = model.fit(train_data, train_labels, epochs=3, validation_split=0.2)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(training.history['loss'])\n",
    "plt.plot(training.history['vall_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오버피팅 전에 최적의 파라미터를 찾는 법."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# This checkpoint object will store the model parameters in the file \"weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint('weights.hdf5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "# Store in a list to be used during training\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Fit the model on a training set, using the checkpoint as a callback\n",
    "model.fit(train_data, train_labels, validation_split=0.2, epochs=3, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매 epoch가 끝날때마다 수행됨.  \n",
    "weights를 저장. initialize하면 weights.hdf5라는 파일이 생성.  \n",
    "val_loss를 monitor해서 validation loss가 나아지면(낮아지면) 그 때 weights 값을 덮어씀.  \n",
    "즉 val_loss가 가장 낮을때의 weights가 저장됨.  \n",
    "checkpoint object는 리스트로 저장해서 model.fit의 callbacks 파라미터로 넣어줌.  \n",
    "fitting이 완료되면 best parameters가 저장.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading stored parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('weights.hdf5')\n",
    "model.predict_classes(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일단 모델을 다시 똑같이 초기화 하고 load_weights 메소드를 이용하여 최적의 파라미터를 불러옴."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the learning curves\n",
    "During learning, the model will store the loss function evaluated in each epoch.  \n",
    "Looking at the learning curves can tell us quite a bit about the learning process.  \n",
    "In this exercise, you will plot the learning and validation loss curves for a model that you will train.  \n",
    "  \n",
    "#### Instructions\n",
    "Fit the model to the training data (train_data).  \n",
    "Use a validation split of 20%, 3 epochs and batch size of 10.  \n",
    "Plot the training loss.  \n",
    "Plot the validation loss.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Train the model and store the training object\n",
    "training = model.fit(train_data, train_labels, validation_split=0.2, epochs=3, batch_size=10)\n",
    "\n",
    "# Extract the history from the training object\n",
    "history = training.history\n",
    "\n",
    "# Plot the training loss \n",
    "plt.plot(history['loss'])\n",
    "# Plot the validation loss\n",
    "plt.plot(history['val_loss'])\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using stored weights to predict in a test set\n",
    "Model weights stored in an hdf5 file can be reused to populate an untrained model.  \n",
    "Once the weights are loaded into this model, it behaves just like a model that has been trained to reach these weights.  \n",
    "For example, you can use this model to make predictions from an unseen data set (e.g. test_data).  \n",
    "  \n",
    "#### Instructions\n",
    "Load the weights from a file called 'weights.hdf5'.  \n",
    "Predict the classes of the first three images from test_data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the weights from file\n",
    "model.load_weights('weights.hdf5')\n",
    "\n",
    "# Predict from the first three images in the test data\n",
    "model.predict(test_data[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we prevent over-fitting and make the best out of our training data?  \n",
    "One of the strategies that has proven effective is regularization.  \n",
    "  \n",
    "Here we'll discuss two strategies for regularization of convolutional neural networks.  \n",
    "### 1. The first strategy is called **\"dropout\"**. In each learning step:\n",
    "- Select a subset of the units\n",
    "    - We choose a random subset of the units in a layer\n",
    "- Ignore it in the forward pass and in the back-propagation of error\n",
    "    - Ignored both on the forward pass throught the network, as well as in the back-propagation stage.\n",
    "- 2014년에 처음 소개..\n",
    "- Dropout allows us to train many different networks on different parts of the data.\n",
    "- If part of the network becomes too sensitive to some noise in the data, other parts will compensate for this, because they havent' seen the samples with this noise.\n",
    "- It also helps prevent different units in the network from becoming overly correlated in their activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout in Keras\n",
    "In Keras, dropout i simplemented as a layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , Flatten, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(5, kernel_size=3, activation='relu', input_shape=(img_rows, img_cols, 1)))\n",
    "\n",
    "model.add(Dropout(0.25))   # Proportion of the units in the layer to ignore in each learning step.\n",
    "\n",
    "model.add(Conv2D(15, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Batch Normalization\n",
    "- Rescale the outputs\n",
    "    - Takes the output of a particular layer\n",
    "    - and rescales it so that it always has 0 mean and standard deviation of 1 in every batch of training.\n",
    "- 2015년 Sergey loffe and Christian Szegedy의 논문에서 제안됨.\n",
    "- This algorithm solves the problem where different batches of input might produce wildly different distributions of outputs in any given layer in the network.\n",
    "- Because the adjustments to the weights through back-propagation depends on the activation of the units in every step of learning, this means that the network may progress very slowly through training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization in Keras\n",
    "Batch normalization is also implented as another type of layer that can be added after each one of the layers whose output should be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(5, kernel_size=3, activation='relu', input_shape=(img_rows, img_cols, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(15, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be careful when using them together!\n",
    "- The disharmony between dropout and batch normalization.\n",
    "    - Dropout slows down learning, making it more incremental and careful,\n",
    "    - Batch Normalization tends to make learning go faster.\n",
    "    - Their effects together may in fact counter each other, \n",
    "    - Networks sometimes perform worse when both of these methods are used together than they would if neither were used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding dropout to your network\n",
    "Dropout is a form of regularization that removes a different random subset of the units in a layer in each round of training.  \n",
    "In this exercise, we will add dropout to the convolutional neural network that we have used in previous exercises:  \n",
    "  \n",
    "Convolution (15 units, kernel size 2, 'relu' activation)  \n",
    "Dropout (20%)  \n",
    "Convolution (5 units, kernel size 2, 'relu' activation)  \n",
    "Flatten  \n",
    "Dense (3 units, 'softmax' activation)  \n",
    "A Sequential model along with Dense, Conv2D, Flatten, and Dropout objects are available in your workspace.  \n",
    "  \n",
    "#### Instructions\n",
    "Add dropout applied to the first layer with 20%.  \n",
    "Add a flattening layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add a convolutional layer\n",
    "model.add(Conv2D(15, kernel_size=2, activation='relu', \n",
    "                 input_shape=(img_rows, img_cols, 1)))\n",
    "\n",
    "# Add a dropout layer\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add another convolutional layer\n",
    "model.add(Conv2D(5, kernel_size=2, activation='relu'))\n",
    "\n",
    "# Flatten and feed to output layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add batch normalization to your network\n",
    "Batch normalization is another form of regularization that rescales the outputs of a layer to make sure that they have mean 0 and standard deviation 1.  \n",
    "In this exercise, we will add batch normalization to the convolutional neural network that we have used in previous exercises:  \n",
    "\n",
    "```\n",
    "Convolution (15 units, kernel size 2, 'relu' activation)\n",
    "Batch normalization\n",
    "Convolution (5 unites, kernel size 2, 'relu' activation)\n",
    "Flatten\n",
    "Dense (3 units, 'softmax' activation)\n",
    "A Sequential model along with Dense, Conv2D, Flatten, and Dropout objects are available in your workspace.\n",
    "```\n",
    "  \n",
    "#### Instructions\n",
    "Add the first convolutional layer.  \n",
    "You can use the img_rows and img_cols objects available in your workspace to define the input_shape of this layer.  \n",
    "Add batch normalization applied to the outputs of the first layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add a convolutional layer\n",
    "model.add(Conv2D(15, kernel_size=2, activation='relu', input_shape=(img_rows, img_cols, 1)))\n",
    "\n",
    "# Add batch normalization layer\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Add another convolutional layer\n",
    "model.add(Conv2D(5, kernel_size=2, activation='relu'))\n",
    "\n",
    "# Flatten and feed to output layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the model\n",
    "One of the main criticisms of convolutional neural networks is that hey are **\"Black boxes\"** and that even when they work very well, it is hard to understand why they work so well.  \n",
    "Many efforts are being made to improve the interpretability of neuralnetworks, and this field is likely to evolve rapidly in the next few years.   \n",
    "One of the major thrusts of this evolution is that people are interested in visualizing what different parts of the network are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting layers\n",
    "Once a model is constructed and compiled, it will store its layers in an attribute called \"layers\".  \n",
    "This is a list of layer objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[<keras.layers.convolutional.Conv2D at 0x109f10c18>,\n",
    " <keras.layers.convolutional.Conv2D at 0x109ec5ba8>,\n",
    " <keras.layers.core.Flatten at 0x1221ffcc0>,\n",
    " <keras.layers.core.Dense at 0x1221ffef0>]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting model weights\n",
    "If we want to look at the first convolutional layer, we can pull it out by indexing the first item in this list.  \n",
    "The weights for this layer are accessible through the get_weights() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1 = model.layers[0]\n",
    "weights1 = conv1.get_weights()\n",
    "len(weights1)  # 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method returns a list with two items.  \n",
    "1. The first item in this list is an array that holds the values of the weights for the convolutional kernels for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernels1 = weights1[0]\n",
    "kernels1.shape # (3, 3, 1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernels array has the shape 3 by 3 by 1 by 5.  \n",
    "The **first 2 dimensions** denote the **kernel size**.  \n",
    "This network was initialized with kernel size of 3  \n",
    "The **third dimension** denotes **the number of channels in the kernels**.  \n",
    "This is one because the network was looking at black and white data.  \n",
    "The **last dimension** denotes **the number of kernels in this layer**: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel1_1 = kernels1[:, :, 0, 0]\n",
    "kernel1_1.shape # (3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pull out the first kernel in this layer, we would use the index 0 into the last dimension.  \n",
    "Because there is only one channel, we can also index on the channel dimension, to collapse that dimension.  \n",
    "This would return the 3 by 3 array containing this convolutional kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the kernel\n",
    "We can then visualize this kernel directly, but understanding what kinds of features this kernel is responding to may be hard just from direct observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(kernel1_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/03f3b5f99d57f9e1ad9d7b10504e987925b265ae/kernel1_1.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the kernel responses\n",
    "To understand what this kernel does, it might sometimes be even more useful to convolve one of the images from our test set with this kernel and see what aspects of the image are emphasized by this kernel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_image = test_data[3, :, :, 0]   # The fourth image from the test_set\n",
    "plt.imshow(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/eaab5e32e6e2007474d61f31180097f9958aad09/test_img3.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_image = convolution(test_image, kernel1_1)\n",
    "plt.imshow(filtered_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convolve it with the kernel using the function that we created previously, and create a filtered image that is the result of this convolution.  \n",
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/13f309b9d8acd32a3799654104ca600f2b90d832/conv1_1.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This filter seems to like the external edges of this image on the left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 kernel, 사진에 대해 반복해서 확인하면서 영감 얻기.. 네트워크를 해석하는데 이용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "### Extracting a kernel from a trained network\n",
    "One way to interpret models is to examine the properties of the kernels in the convolutional layers.  \n",
    "In this exercise, you will extract one of the kernels from a convolutional neural network with weights that you saved in a hdf5 file.\n",
    "  \n",
    "#### Instructions\n",
    "Load the weights into the model from the file weights.hdf5.  \n",
    "Get the first convolutional layer in the model from the layers attribute.  \n",
    "Use the .get_weights() method to extract the weights from this layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the weights into the model\n",
    "model.load_weights('weights.hdf5')\n",
    "\n",
    "# Get the first convolutional layer from the model\n",
    "c1 = model.layers[0]\n",
    "\n",
    "# Get the weights of the first convolutional layer\n",
    "weights1 = c1.get_weights()\n",
    "\n",
    "# Pull out the first channel of the first kernel in the first layer\n",
    "kernel = weights1[0][...,0, 0]\n",
    "print(kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[[ 0.03504268  0.4328133 ]\n",
    " [-0.17416623  0.4680562 ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape of the weights\n",
    "A Keras neural network stores its layers in a list called model.layers.  \n",
    "For the convolutional layers, you can get the weights using the .get_weights() method.  \n",
    "This returns a list, and the first item in this list is an array representing the weights of the convolutional kernels.  \n",
    "If the shape of this array is (2, 2, 1, 5), what does the first number `2` represent?  \n",
    "  \n",
    "#### Instructions\n",
    "Possible Answers  \n",
    "1. There are 2 channels in black and white images.  \n",
    "2. The kernel size is 2 by 2.\n",
    "3. The model used a convolutional unit with 2 dimensions.\n",
    "4. There are 2 convolutional layers in the network.\n",
    "  \n",
    "The answer is 2, each of the 2s in this shape is one of the dimensions of the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing kernel responses\n",
    "One of the ways to interpret the weights of a neural network is to see how the kernels stored in these weights \"see\" the world.  \n",
    "That is, what properties of an image are emphasized by this kernel.  \n",
    "In this exercise, we will do that by convolving an image with the kernel and visualizing the result.  \n",
    "Given images in the test_data variable, a function called extract_kernel() that extracts a kernel from the provided network, and the function called convolution() that we defined in the first chapter, extract the kernel, load the data from a file and visualize it with matplotlib.  \n",
    "  \n",
    "A deep CNN model, a function convolution(), along with the kernel you extracted in an earlier exercise is available in your workspace. \n",
    "  \n",
    "#### Instructions\n",
    "Use the convolution() function to convolve the extracted kernel with the first channel of the fourth item in the image array.  \n",
    "Visualize the resulting convolution with imshow()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convolve with the fourth image in test_data\n",
    "out = convolution(test_data[3, :, :, 0], kernel)\n",
    "\n",
    "# Visualize the result\n",
    "plt.imshow(out)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data:image/svg+xml;base64,<?xml version="1.0" encoding="utf-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
  "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Created with matplotlib (http://matplotlib.org/) -->
<svg height="239.996pt" version="1.1" viewBox="0 0 338 239.996" width="338pt" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
 <defs>
  <style type="text/css">
*{stroke-linecap:butt;stroke-linejoin:round;}
  </style>
 </defs>
 <g id="figure_1">
  <g id="patch_1">
   <path d="M 0 239.996 
L 338 239.996 
L 338 0 
L 0 0 
z
" style="fill:#ffffff;"/>
  </g>
  <g id="axes_1">
   <g id="patch_2">
    <path d="M 69.891063 212.517875 
L 268.108937 212.517875 
L 268.108937 14.3 
L 69.891063 14.3 
z
" style="fill:#ffffff;"/>
   </g>
   <g clip-path="url(#p0e502225fc)">
    <image height="199" id="imageea9a4b6659" transform="scale(1 -1)translate(0 -199)" width="199" x="69.891063" xlink:href="data:image/png;base64,
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" y="-13.517875"/>
   </g>
   <g id="matplotlib.axis_1">
    <g id="xtick_1">
     <g id="line2d_1">
      <defs>
       <path d="M 0 0 
L 0 3.5 
" id="mb471ad144d" style="stroke:#000000;stroke-width:0.8;"/>
      </defs>
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="73.430667" xlink:href="#mb471ad144d" y="212.517875"/>
      </g>
     </g>
     <g id="text_1">
      <!-- 0 -->
      <defs>
       <path d="M 31.78125 66.40625 
Q 24.171875 66.40625 20.328125 58.90625 
Q 16.5 51.421875 16.5 36.375 
Q 16.5 21.390625 20.328125 13.890625 
Q 24.171875 6.390625 31.78125 6.390625 
Q 39.453125 6.390625 43.28125 13.890625 
Q 47.125 21.390625 47.125 36.375 
Q 47.125 51.421875 43.28125 58.90625 
Q 39.453125 66.40625 31.78125 66.40625 
z
M 31.78125 74.21875 
Q 44.046875 74.21875 50.515625 64.515625 
Q 56.984375 54.828125 56.984375 36.375 
Q 56.984375 17.96875 50.515625 8.265625 
Q 44.046875 -1.421875 31.78125 -1.421875 
Q 19.53125 -1.421875 13.0625 8.265625 
Q 6.59375 17.96875 6.59375 36.375 
Q 6.59375 54.828125 13.0625 64.515625 
Q 19.53125 74.21875 31.78125 74.21875 
z
" id="DejaVuSans-30"/>
      </defs>
      <g transform="translate(70.249417 227.116312)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-30"/>
      </g>
     </g>
    </g>
    <g id="xtick_2">
     <g id="line2d_2">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="108.826717" xlink:href="#mb471ad144d" y="212.517875"/>
      </g>
     </g>
     <g id="text_2">
      <!-- 5 -->
      <defs>
       <path d="M 10.796875 72.90625 
L 49.515625 72.90625 
L 49.515625 64.59375 
L 19.828125 64.59375 
L 19.828125 46.734375 
Q 21.96875 47.46875 24.109375 47.828125 
Q 26.265625 48.1875 28.421875 48.1875 
Q 40.625 48.1875 47.75 41.5 
Q 54.890625 34.8125 54.890625 23.390625 
Q 54.890625 11.625 47.5625 5.09375 
Q 40.234375 -1.421875 26.90625 -1.421875 
Q 22.3125 -1.421875 17.546875 -0.640625 
Q 12.796875 0.140625 7.71875 1.703125 
L 7.71875 11.625 
Q 12.109375 9.234375 16.796875 8.0625 
Q 21.484375 6.890625 26.703125 6.890625 
Q 35.15625 6.890625 40.078125 11.328125 
Q 45.015625 15.765625 45.015625 23.390625 
Q 45.015625 31 40.078125 35.4375 
Q 35.15625 39.890625 26.703125 39.890625 
Q 22.75 39.890625 18.8125 39.015625 
Q 14.890625 38.140625 10.796875 36.28125 
z
" id="DejaVuSans-35"/>
      </defs>
      <g transform="translate(105.645467 227.116312)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-35"/>
      </g>
     </g>
    </g>
    <g id="xtick_3">
     <g id="line2d_3">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="144.222766" xlink:href="#mb471ad144d" y="212.517875"/>
      </g>
     </g>
     <g id="text_3">
      <!-- 10 -->
      <defs>
       <path d="M 12.40625 8.296875 
L 28.515625 8.296875 
L 28.515625 63.921875 
L 10.984375 60.40625 
L 10.984375 69.390625 
L 28.421875 72.90625 
L 38.28125 72.90625 
L 38.28125 8.296875 
L 54.390625 8.296875 
L 54.390625 0 
L 12.40625 0 
z
" id="DejaVuSans-31"/>
      </defs>
      <g transform="translate(137.860266 227.116312)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-31"/>
       <use x="63.623047" xlink:href="#DejaVuSans-30"/>
      </g>
     </g>
    </g>
    <g id="xtick_4">
     <g id="line2d_4">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="179.618815" xlink:href="#mb471ad144d" y="212.517875"/>
      </g>
     </g>
     <g id="text_4">
      <!-- 15 -->
      <g transform="translate(173.256315 227.116312)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-31"/>
       <use x="63.623047" xlink:href="#DejaVuSans-35"/>
      </g>
     </g>
    </g>
    <g id="xtick_5">
     <g id="line2d_5">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="215.014864" xlink:href="#mb471ad144d" y="212.517875"/>
      </g>
     </g>
     <g id="text_5">
      <!-- 20 -->
      <defs>
       <path d="M 19.1875 8.296875 
L 53.609375 8.296875 
L 53.609375 0 
L 7.328125 0 
L 7.328125 8.296875 
Q 12.9375 14.109375 22.625 23.890625 
Q 32.328125 33.6875 34.8125 36.53125 
Q 39.546875 41.84375 41.421875 45.53125 
Q 43.3125 49.21875 43.3125 52.78125 
Q 43.3125 58.59375 39.234375 62.25 
Q 35.15625 65.921875 28.609375 65.921875 
Q 23.96875 65.921875 18.8125 64.3125 
Q 13.671875 62.703125 7.8125 59.421875 
L 7.8125 69.390625 
Q 13.765625 71.78125 18.9375 73 
Q 24.125 74.21875 28.421875 74.21875 
Q 39.75 74.21875 46.484375 68.546875 
Q 53.21875 62.890625 53.21875 53.421875 
Q 53.21875 48.921875 51.53125 44.890625 
Q 49.859375 40.875 45.40625 35.40625 
Q 44.1875 33.984375 37.640625 27.21875 
Q 31.109375 20.453125 19.1875 8.296875 
z
" id="DejaVuSans-32"/>
      </defs>
      <g transform="translate(208.652364 227.116312)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-32"/>
       <use x="63.623047" xlink:href="#DejaVuSans-30"/>
      </g>
     </g>
    </g>
    <g id="xtick_6">
     <g id="line2d_6">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="250.410913" xlink:href="#mb471ad144d" y="212.517875"/>
      </g>
     </g>
     <g id="text_6">
      <!-- 25 -->
      <g transform="translate(244.048413 227.116312)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-32"/>
       <use x="63.623047" xlink:href="#DejaVuSans-35"/>
      </g>
     </g>
    </g>
   </g>
   <g id="matplotlib.axis_2">
    <g id="ytick_1">
     <g id="line2d_7">
      <defs>
       <path d="M 0 0 
L -3.5 0 
" id="m79d6d37eb6" style="stroke:#000000;stroke-width:0.8;"/>
      </defs>
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="69.891063" xlink:href="#m79d6d37eb6" y="17.839605"/>
      </g>
     </g>
     <g id="text_7">
      <!-- 0 -->
      <g transform="translate(56.528563 21.638824)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-30"/>
      </g>
     </g>
    </g>
    <g id="ytick_2">
     <g id="line2d_8">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="69.891063" xlink:href="#m79d6d37eb6" y="53.235654"/>
      </g>
     </g>
     <g id="text_8">
      <!-- 5 -->
      <g transform="translate(56.528563 57.034873)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-35"/>
      </g>
     </g>
    </g>
    <g id="ytick_3">
     <g id="line2d_9">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="69.891063" xlink:href="#m79d6d37eb6" y="88.631703"/>
      </g>
     </g>
     <g id="text_9">
      <!-- 10 -->
      <g transform="translate(50.166063 92.430922)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-31"/>
       <use x="63.623047" xlink:href="#DejaVuSans-30"/>
      </g>
     </g>
    </g>
    <g id="ytick_4">
     <g id="line2d_10">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="69.891063" xlink:href="#m79d6d37eb6" y="124.027752"/>
      </g>
     </g>
     <g id="text_10">
      <!-- 15 -->
      <g transform="translate(50.166063 127.826971)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-31"/>
       <use x="63.623047" xlink:href="#DejaVuSans-35"/>
      </g>
     </g>
    </g>
    <g id="ytick_5">
     <g id="line2d_11">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="69.891063" xlink:href="#m79d6d37eb6" y="159.423801"/>
      </g>
     </g>
     <g id="text_11">
      <!-- 20 -->
      <g transform="translate(50.166063 163.22302)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-32"/>
       <use x="63.623047" xlink:href="#DejaVuSans-30"/>
      </g>
     </g>
    </g>
    <g id="ytick_6">
     <g id="line2d_12">
      <g>
       <use style="stroke:#000000;stroke-width:0.8;" x="69.891063" xlink:href="#m79d6d37eb6" y="194.81985"/>
      </g>
     </g>
     <g id="text_12">
      <!-- 25 -->
      <g transform="translate(50.166063 198.619069)scale(0.1 -0.1)">
       <use xlink:href="#DejaVuSans-32"/>
       <use x="63.623047" xlink:href="#DejaVuSans-35"/>
      </g>
     </g>
    </g>
   </g>
   <g id="patch_3">
    <path d="M 69.891063 212.517875 
L 69.891063 14.3 
" style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"/>
   </g>
   <g id="patch_4">
    <path d="M 268.108937 212.517875 
L 268.108937 14.3 
" style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"/>
   </g>
   <g id="patch_5">
    <path d="M 69.891063 212.517875 
L 268.108937 212.517875 
" style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"/>
   </g>
   <g id="patch_6">
    <path d="M 69.891063 14.3 
L 268.108937 14.3 
" style="fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;"/>
   </g>
  </g>
 </g>
 <defs>
  <clipPath id="p0e502225fc">
   <rect height="198.217875" width="198.217875" x="69.891063" y="14.3"/>
  </clipPath>
 </defs>
</svg>
\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What did we learn?\n",
    "- Image classification\n",
    "- Convolutions\n",
    "- Reducing the number of parameters\n",
    "    - Tweaking your convolutions\n",
    "    - Adding pooling layers\n",
    "- Improving your network\n",
    "    - Regularization\n",
    "- Understanding your network\n",
    "    - Monitoring learning\n",
    "    - Interpreting the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model interpretation\n",
    "https://distill.pub/2017/feature-visualization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What next?\n",
    "- Even deeper networks\n",
    "### Residual networks\n",
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/8bd8fc985a4e9ed8c3081c05a874210e826220c4/resnet.png\" width=\"400\">\n",
    "These include connections that skip over several layers, and they are called residual networks because the network will use this skipped connectino to compute a difference between the input of a stack of layers and their output.  \n",
    "Learning this difference, or residual, turns out to often be easier than learning the output.  \n",
    "This means that these networks have been surprisingly effective at tasks such as classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "In this approach an already-trained network is adapted to a new task.  \n",
    "It's a great strategy for cases where you don't have a lot of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Convolutional Networks\n",
    "Take an image as input and produce another image as output.  \n",
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/9191cd9da78093f76937101eea8aed1cda994c2c/fully-convolutional.png\" width=\"500\">\n",
    "For example, these networks can be used to find the part of an image that contains a particular kind of object doing segmentation rather than classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Adversarial Networks(GAN)\n",
    "<img src=\"https://assets.datacamp.com/production/repositories/1820/datasets/0625067ec02a09a3557dc5481feb5a96ec18f59f/gan.png\" width=\"500\">\n",
    "These complex architectures can be used to train a network to create new images that didn't exist before."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
