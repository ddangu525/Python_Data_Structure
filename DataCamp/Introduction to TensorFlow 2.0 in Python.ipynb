{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants and variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is TensorFlow?\n",
    "- Open-soure library for graph-based numerical computation\n",
    "    - Developed by the Google Brain Team\n",
    "- Low and high level APIs\n",
    "    - Addition, multiplication, differentiation\n",
    "    - Machine learning models\n",
    "- Important changes in TensorFlow 2.0\n",
    "    - Eager execution by default\n",
    "    - Model building with Keras and Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a tensor?\n",
    "- Generalization of vectors and matrices\n",
    "- Collection of numbers\n",
    "- Specific shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining tensors in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 0D Tensor\n",
    "d0 = tf.ones((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1D Tensor\n",
    "d1 = tf.ones((2,))\n",
    "\n",
    "# 2D Tensor\n",
    "d2 = tf.ones((2, 2))\n",
    "\n",
    "# 3D Tensor\n",
    "d3 = tf.ones((2, 2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ones_3:0\", shape=(2, 2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-68ffdd786830>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Print the 3D tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "# Print the 3D tensor\n",
    "# print(d3.numpy())    tensorflow 2.0부터 있는듯..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining constants in TensorFlow\n",
    "- A constant is the simplest category of tensor\n",
    "    - Not trainable\n",
    "    - Can have any dimention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a 2x3 constant.\n",
    "a = constant(3, shape=[2, 3])\n",
    "\n",
    "# Define a 2x2 constant.\n",
    "b = constant([1, 2, 3, 4], shape=[2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using convenience functions to define constants\n",
    "\n",
    "```\n",
    "Operation           |  Example\n",
    "--------------------------------------------\n",
    "tf.constant()         constant([1, 2, 3])\n",
    "tf.zeros()            zeros([2, 2])\n",
    "tf.zeros_like()       zeros_like(input_tensor)\n",
    "tf.ones()             ones([2, 2])\n",
    "tf.ones_like()        ones_like(input_tensor)\n",
    "tf.fill()             fill([3, 3], 7)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining and initializing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a variable\n",
    "a0 = tf.Variable([1, 2, 3, 4, 5, 6], dtype=tf.float32)\n",
    "a1 = tf.Variable([1, 2, 3, 4, 5, 6], dtype=tf.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a constant\n",
    "b = tf.constant(2, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute their product\n",
    "c0 = tf.multiply(a0, b)\n",
    "c1 = a0 * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Operations\n",
    "Graph Based.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import constant, add\n",
    "\n",
    "# Define 0-dimensional tensors\n",
    "A0 = constant([1])\n",
    "B0 = constant([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define 1-dimensional tensors\n",
    "A1 = constant([1, 2])\n",
    "B1 = constant([3, 4])\n",
    "\n",
    "# Define 2-dimensional tensors\n",
    "A2 = constant([[1, 2], [3, 4]])\n",
    "B2 = constant([[5, 6], [7, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Add:0\", shape=(1,), dtype=int32)\n",
      "Tensor(\"Add_1:0\", shape=(2,), dtype=int32)\n",
      "Tensor(\"Add_2:0\", shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Perform tensor addition with add()\n",
    "C0 = add(A0, B0)\n",
    "C1 = add(A1, B1)\n",
    "C2 = add(A2, B2)\n",
    "\n",
    "print(C0)\n",
    "print(C1)\n",
    "print(C2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The add() operation performs **element-wise addtion** with two tensors.\n",
    "- Elemen-wise addition requires both tensors to have the same shape.\n",
    "- Overloaded 돼있기 때문에 +기호로도 사용 가능??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add:0\", shape=(1,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(A0 + B0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Element-wise multiplication performed using multiply() operation.\n",
    "- Matrix multiplication performed with matmul() operator.\n",
    "    - matmul(A, B) operation multiplies A by B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import ones, matmul, multiply\n",
    "\n",
    "# Define tensors\n",
    "A0 = ones(1)\n",
    "A31 = ones([3, 1])\n",
    "A34 = ones([3, 4])\n",
    "A43 = ones([4, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing over tensor dimensions\n",
    "- The reduce_sum() operator sums over the dimensions of a tensor\n",
    "    - reduce_sum(A) sums over all dimensions of A\n",
    "    - reduce_sum(A, i) sums over dimension i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import ones, reduce_sum\n",
    "\n",
    "# Define a 2x3x4 tensor of ones\n",
    "A = ones([2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sum over all dimensions\n",
    "B = reduce_sum(A)\n",
    "\n",
    "# Sum over dimensions 0, 1, and 2\n",
    "B0 = reduce_sum(A, 0)\n",
    "B1 = reduce_sum(A, 1)\n",
    "B2 = reduce_sum(A, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Sum_3:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Sum_4:0\", shape=(3, 4), dtype=float32)\n",
      "Tensor(\"Sum_5:0\", shape=(2, 4), dtype=float32)\n",
      "Tensor(\"Sum_6:0\", shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(B)\n",
    "print(B0)\n",
    "print(B1)\n",
    "print(B2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Operation     |                Use\n",
    "--------------|-----------------------------------------------------------------\n",
    "gradient()    |    Computes the slope of a function at a point\n",
    "reshape()     |    Reshapes a tensor(e.g. 10x10 to 100x1)\n",
    "random()      |    Populates tensor with entries drawn from a probability distribution\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the optimum\n",
    "- In many problems, we will want to fine the optimum of a function.\n",
    "    - Minimum: Lowest value of a loss function.\n",
    "    - Maximum: Highest value of objective function.\n",
    "- We can do this using the gradient() operation.\n",
    "    - Optimum: Find a point where gradient = 0\n",
    "    - Minimum: Change in gradient > 0\n",
    "    - Maximum: Change in gradient < 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'GradientTape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-85c391879ab5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Define y within instance of GradientTape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'GradientTape'"
     ]
    }
   ],
   "source": [
    "# Define x\n",
    "x = tf.Variable(-1.0)\n",
    "\n",
    "# Define y within instance of GradientTape... 2.0버전에 있는것..\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = tf.multiply(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the gradient of y at x = -1\n",
    "g = tape.gradient(y, x)\n",
    "print(g.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images as tensors\n",
    "#### How to reshape a grayscale image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'random'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-dd3760680c8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Generage grayscale image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'int32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Reshape grayscale image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'random'"
     ]
    }
   ],
   "source": [
    "# Generage grayscale image\n",
    "gray = tf.random.uniform([2, 2], maxval=255, dtype='int32')    # tf.random도 2.0버전에 생긴거네..\n",
    "\n",
    "# Reshape grayscale image\n",
    "gray = tf.reshape(gray, [2*2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to reshape a color image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generage color image\n",
    "color = tf.random.uniform([2, 2, 3], maxval=255, dtype='int32')\n",
    "\n",
    "# Reshape color image\n",
    "color = tf.reshape(color, [2*2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing data for use in TensorFlow\n",
    "- Data can be imported using tensorflow\n",
    "    - Useful for managing complex pipelines\n",
    "- Simpler option\n",
    "    - Import data using pandas\n",
    "    - Convert data to numpy array\n",
    "    - Use in tensorflow without modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from csv\n",
    "housing = pd.read_csv('kc_housing.csv')\n",
    "\n",
    "# Convert to numpy array\n",
    "housing = np.array(housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pandas also has methods for handling data in other formats\n",
    "    - E.g. read_json(), read_html(), read_excel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using mixed type datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load KC dataset\n",
    "housing = pd.read_csv('kc_housing.csv')\n",
    "\n",
    "# Convert price column to float32\n",
    "price = np.array(housing['price'], np.float32)\n",
    "\n",
    "# Convert waterfront column to Boolean\n",
    "waterfront = np.array(housing['waterfront'], np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cast approach\n",
    "price = tf.cast(housing['price'], tf.float32)\n",
    "waterfront = tf.cast(housing['waterfront'], tf.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fundamental tenorflow operation\n",
    "    - Used to train a model\n",
    "    - Measure of model fit\n",
    "- Higher value -> worse fit\n",
    "    - Minimize the loss function\n",
    "  \n",
    "- TensorFlow has operations for common loss functions\n",
    "    - Mean Squared Error(MSE)\n",
    "    - Mean Absolute Error(MAE)\n",
    "    - Huber Error\n",
    "- Loss functions are accessible from tf.keras.losses()\n",
    "    - tf.keras.losses.mse()\n",
    "    - tf.keras.losses.mae()\n",
    "    - tf.keras.losses.Huber()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Other loss functions\n",
    "    - Mean Absolute Percentage Error(MAPE) / tf.keras.losses.mape()\n",
    "    - Mean Squared Logarighmic Error(MSLE) / tf.keras.losses.msle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the MSE loss\n",
    "loss = tf.keras.losses.mse(target, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a loss function to compute the MSE\n",
    "def loss_function(intercept, slope, target, features):\n",
    "    # Compute the predictions for a linear model\n",
    "    predictions = intercept + features * slope\n",
    "    # Return the loss\n",
    "    return tf.keras.losses.mse(target, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the loss for given input data and model parameters\n",
    "loss_function(intercept, slope, prices, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A linear regression model assumes a linear relationship:\n",
    "    - price = intercept + size \\* slope + error\n",
    "- This is an example of a univariate regression.\n",
    "    - There is only on feature, size\n",
    "- Multiple regression models have more than one feature.\n",
    "    - E.g. size and location    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the targets and features\n",
    "price = np.array(housing['price'], np.float32)\n",
    "size = np.array(housing['sqft_living'], np.float32)\n",
    "\n",
    "# Define the intercept and slope\n",
    "intercept = tf.Variable(0.1, np.float32)\n",
    "slope = tf.Variabel(0.1, np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute the predicted values and loss function\n",
    "def loss_function(intercept, slope, size, price):\n",
    "    predictions = intercept + size * slope\n",
    "    return tf.keras.losses.mse(price, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define an optimization operation\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Minimize the loss function and print the loss\n",
    "for j in range(1000):\n",
    "    opt.minimize(lambda: loss_function(intercept, slope, size, price), var_list=[interept, slope])\n",
    "    print(loss_function(intercept, slope, size, price))\n",
    "    \n",
    "# Print the trained parameters\n",
    "print(intercept.numpy(), slope.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is batch training?\n",
    "데이터를 여러개의 batch로 나눠서 한번에 하나의 batch만 처리하는것.  \n",
    "- pd.read_csv() allows us to load data in batches\n",
    "    - Avoid loading entire dataset\n",
    "    - chunksize parameter provides batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data in batches\n",
    "for batch in pd.read_csv('kc_housing.csv', chunksize=100):\n",
    "    # Extract price column\n",
    "    price = np.array(batch['price'], np.float32)\n",
    "    # Extract size column \n",
    "    size = np.array(batch['size'], np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a linear model in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intercept = tf.Variable(0.1, tf.float32)\n",
    "slope = tf.Variable(p.1, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute predicted values and return loss function\n",
    "def loss_function(intercept, slope, featuresm, target):\n",
    "    predictions = intercept + features * slope\n",
    "    return tf.keras.losses.mse(target, predictions)\n",
    "\n",
    "# Define optimization operation\n",
    "opt = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data in batches from pandas\n",
    "for batch in pd.read_csv('kc_housing.csv', chunksize=100):\n",
    "    # Extract the target and feature column\n",
    "    price_batch = np.array(batch['price'], np.float32)\n",
    "    size_batch = np.array(batch['size'], np.float32)\n",
    "    # Minimize the loss function\n",
    "    opt.minimize(lambda : loss_function(intercept, slope, size_batch, price_batch), var_list = [intercept, slope])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print parameter values\n",
    "print(intercept.numpy(), slope.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full sample versus batch training\n",
    "1. Full Sample\n",
    "    1. One step per epoch\n",
    "    2. Accepts dataset without modification\n",
    "    3. Limited by memory\n",
    "2. Batch Training\n",
    "    1. Multiple steps per epoch\n",
    "    2. Requires division of dataset\n",
    "    3. No limit on dataset size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The linear regression model\n",
    "Linear combination으로 표현.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a neural network?\n",
    "Input layer(Features) -> Hidden layers -> output layer(Prediction)  \n",
    "  \n",
    "- Dense layer: 이전의 모든 노드로부터 연결돼있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A trivial dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define input data\n",
    "inputs = tf.constant([[1, 35]])\n",
    "\n",
    "# Define weights\n",
    "weights = tf.Variable([[-0.05], [-0.01]])\n",
    "\n",
    "# Multiply inputs by the weights\n",
    "product = tf.matmul(inputs, weights)\n",
    "\n",
    "# Define dense layer\n",
    "dense = tf.keras.activations.sigmoid(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define input layer\n",
    "inputs = tf.constant(data, tf.float32)\n",
    "\n",
    "# Define first dense layer\n",
    "dense1 = tf.keras.layers.Dense(10, activation='sigmoid')(inputs)  # Number of outgoing node, activation function\n",
    "\n",
    "# Define second dense layer\n",
    "dense2 = tf.keras.layers.Dense(5, activation='sigmoid')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### High-level versus low-level approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- High-level approach\n",
    "    - High-level API operations.   (dense = keras.layers.Dense(10, activation='sigmoid')\n",
    "- Low-level approach\n",
    "    - Linear-algebraic operations (prod = matmul(inputs, weights) // dense = keras.activations.sigmoid(prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Components of a typical hidden layer\n",
    "    - LinearL Matrix multiplication\n",
    "    - Nonlinear: Activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why nonlinearities are important\n",
    "Nonlinearity 관계를 얻기 위해.. linear layer는 여러번 쌓는 의미가 없다. 하나의 선형변환으로 쓸 수 있음.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A simple example\n",
    "# Define example borrower features\n",
    "young, old = 0.3, 0.6\n",
    "low_bill, high_bill = 0.1, 0.5\n",
    "\n",
    "# Compute products and sums\n",
    "young_high = 1.0 * young + 1.0 * high_bill\n",
    "young_low = 1.0 * young + 1.0 * low_bill\n",
    "old_high = 1.0 * old + 1.0 * high_bill\n",
    "old_low = 1.0 * old + 1.0 * low_bill\n",
    "\n",
    "# Print difference for young\n",
    "print(young_high - young_low)\n",
    "\n",
    "# Print difference for old\n",
    "print(old_high - old_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print difference for young after activation is applied\n",
    "print(tf.keras.activations.sigmoid(young_high).numpy() - tf.keras.activations.sigmoid(young_low).numpy())\n",
    "\n",
    "# Print difference for old after activation is applied\n",
    "print(tf.keras.activations.sigmoid(old_high).numpy() - tf.keras.activations.sigmoid(old_low).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The sigmoid activation function\n",
    "- Binary classification\n",
    "- Low-level: tf.keras.activations.sigmoid()\n",
    "- High-level: sigmoid\n",
    "  \n",
    "#### The relu activation function\n",
    "- Hidden layers\n",
    "- Low-level: tf.keras.activations.relu()\n",
    "- High-level: relu\n",
    "  \n",
    "#### The softmax activation function\n",
    "- Output layer( > 2 classes) Multiclass classification\n",
    "- High-level: tf.keras.activations.softmax()\n",
    "- Low-level: softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions in neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define input layer\n",
    "inputs = tf.constant(borrower_features, tf.float32)\n",
    "\n",
    "# Define dense layer 1\n",
    "dense1 = tf.keras.layers.Dense(16, activation='relu')(inputs)\n",
    "\n",
    "# Define dense layer 2\n",
    "dense2 = tf.keras.layers.Dense(8, activation='sigmoid')(dense1)\n",
    "\n",
    "# Define output layer\n",
    "outputs = tf.keras.layers.Dense(4, activation='softmax')(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to find a minimum?\n",
    "경사를 따라 내려가자! Gradient Descent Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SGD(Stochastic gradient descent optimizer)\n",
    "    - tf.keras.optimizers.SGD()\n",
    "    - learning_rate\n",
    "- RMS(Root mean squared propagation optimizer)\n",
    "    - Applies different learning rates to each feature\n",
    "    - tf.keras.optimizers.RMSprop()\n",
    "    - learning_rate\n",
    "    - decay, momentum\n",
    "- Adam(Adaptive moment optimizer)\n",
    "    - tf.keras.optimizers.Adam()\n",
    "    - learning_rate\n",
    "    - beta1\n",
    "    - beta2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A complete example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Compute the predicted values and loss\n",
    "def loss_function(weights):\n",
    "    product = tf.matmul(borrower_features, weights)\n",
    "    predictions = tf.keras.activations.sigmoid(product)\n",
    "    return tf.keras.losses.binary_crossentropy(default, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize the loss function with adam\n",
    "opt = tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.8)\n",
    "opt.minimize(lambda : loss_function(weights), var_list=[weights])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Minima에 빠지지 않도록 초기값, 하이퍼파라미터를 잘 조절해주어야 함!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a network in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random initializers\n",
    "- Often need to initialize thousands of variables\n",
    "    - tf.ones() may perform poorly\n",
    "    - Tedious and difficult to initialize variables individually\n",
    "- Alternatively, draw initial values from distribution\n",
    "    - Random normal\n",
    "    - Uniform\n",
    "    - Glorot initializer. 이건 처음보는데??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing variables in TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define 500x500 random normal variable\n",
    "weights = tf.Variable(tf.random.normal([500, 500]))\n",
    "\n",
    "# Define 500x500 truncated random normal variable.(매우 큰 값이나 매우 작은 값을 버림..)\n",
    "weights = tf.Variable(tf.random.truncated_normal([500, 500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a dense layer with the default initializer\n",
    "dense = tf.keras.layers.Dense(32, activation='relu')\n",
    "\n",
    "# Define a dense layer with the zeros initializer\n",
    "dense = tf.keras.layers.Dense(32, activation='relu', kernel_initializer='zeros')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will train a neural network to predict whether a credit card holder will default.  \n",
    "The features and targets you will use to train your network are available in the Python shell as borrower_features and default.  \n",
    "You defined the weights and biases in the previous exercise.  \n",
    "  \n",
    "Note that output_layer is defined as σ(layer1∗weights2+bias2), where σ is the sigmoid activation,  \n",
    "layer1 is a tensor of nodes for the first hidden dense layer, weight2 is a tensor of weights, and bias2 is the bias tensor.  \n",
    "  \n",
    "The trainable variables are weights1, bias1, weights2, and bias2. Additionally,  \n",
    "the following operations have been imported for you: nn.relu() and keras.layers.Dropout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a rectified linear unit activation function to the first layer.  \n",
    "Apply 25% dropout to layer1.  \n",
    "Pass the target, targets, and the predicted values, layer2, to the cross entropy loss function.  \n",
    "Add the four trainable variables to var_list in the order in which they appear as arguments to loss_function().  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_function(weights1, bias1, weights2, bias2, features, targets):\n",
    "    # Apply relu activation functions to layer 1\n",
    "    layer1 = nn.relu(add(matmul(features, weights1), bias1))\n",
    "    # Apply dropout\n",
    "    dropout = keras.layers.Dropout(0.25)(layer1)\n",
    "    layer2 = nn.sigmoid(add(matmul(dropout, weights2), bias2))\n",
    "    # Pass targets and layers2 to the cross entropy loss\n",
    "    return keras.losses.binary_crossentropy(targets, layer2)\n",
    "  \n",
    "for j in range(0, 30000, 2000):\n",
    "    features, targets = borrower_features[j:j+2000, :], default[j:j+2000, :]\n",
    "    # Complete the optimizer\n",
    "    opt.minimize(lambda: loss_function(weights1, bias1, weights2, bias2, features, targets), var_list=[weights1, bias1, weights2, bias2])\n",
    "    \n",
    "print(weights1.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural networks and overfitting\n",
    "#### Applying dropout\n",
    "과적합을 막기 위해 일부 연결관계를 끊어줌.\n",
    "#### Implementing dropout in a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define input data\n",
    "inputs = np.array(borrower_features, np.float32)\n",
    "\n",
    "# Define dense layer 1\n",
    "dense1 = tf.keras.layers.Dense(32, activation='relu')(inputs)\n",
    "\n",
    "# Define dense layer 2\n",
    "dense2 = tf.keras.layers.Dense(16, activation='relu')(dense1)\n",
    "\n",
    "# Apply dropout operation\n",
    "dropout1 = tf.keras.layers.Dropout(0.25)(dense2)    # 25%의 노드의 연결을 끊음..\n",
    "\n",
    "# Define output layer\n",
    "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(dropout1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining neural networks with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sequential API\n",
    "- Input layer\n",
    "- Hidden layers\n",
    "- Output layer\n",
    "- Ordered in sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a sequential model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Define first hidden layer\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(28*28,)))\n",
    "\n",
    "# Define second hidden layer\n",
    "model.add(keras.layers.Dense(8, activation='relu'))\n",
    "\n",
    "# Define output layer\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile('adam', loss='categorical_crossentropy')  # Multiclass Classification에 이용.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model 1 input layer shape\n",
    "model1_inputs = tf.keras.Input(shape=(28*28,))\n",
    "\n",
    "# Define model 2 input layer shape\n",
    "model2_inputs = tf.keras.Input(shape=(10,))\n",
    "\n",
    "# Define layer 1 for model 1\n",
    "model1_layer1 = tf.keras.layers.Dense(12, activation='relu')(model1_inputs)\n",
    "\n",
    "# Define layer 2 for model 1\n",
    "model1_layer2 = tf.keras.layers.Dense(4, activation='softmax')(model1_layer1)\n",
    "\n",
    "# Define layer 1 for model 2\n",
    "model2_layer1 = tf.keras.layers.Dense(8, activation='relu')(model2_inputs)\n",
    "\n",
    "# Define layer 2 for model 2\n",
    "model2_layer2 = tf.keras.layers.Dense(4, activation='softmax')(model2_layer1)\n",
    "\n",
    "# Merge model 1 and model 2\n",
    "merged = tf.keras.layers.add([model1_layer2, model2_layer2])\n",
    "\n",
    "# Define a functional model\n",
    "model = tf.keras.Model(inputs=[moel1_inputs, model2_inputs], outputs=merged)\n",
    "\n",
    "# Compile the model\n",
    "model.compile('adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, the sequential API will not be sufficiently flexible to accommodate your desired model architecture  \n",
    "and you will need to use the functional API instead.  \n",
    "If, for instance, you want to train two models with different architectures jointly,  \n",
    "you will need to use the functional API to do this. You will use the functional API to merge the two models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and validation with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of training\n",
    "1. Load and clean data\n",
    "2. Define model\n",
    "3. Train and validate model\n",
    "4. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a sequential model\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Define the hidden layer\n",
    "model.add(tf.keras.layers.Dense(16, activation='relu', input_shape=(784,)))\n",
    "\n",
    "# Define the output layer\n",
    "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile('adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Train model\n",
    "model.fit(image_features, image_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The fit() operation\n",
    "- Required arguments\n",
    "    - features\n",
    "    - labels\n",
    "- Many optional arguments\n",
    "    - batch_size\n",
    "    - epochs\n",
    "    - validation_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train model with validation split\n",
    "model.fit(features, labels, epochs=10, validation_split=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Recompile the model with the accuraty metric\n",
    "model.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model with validation split\n",
    "model.fit(features, labels, epochs=10, validation_split=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating models\n",
    "When you train a model, you will typically divide your data into three subsets: train, validation, and test.  \n",
    "During the training phase, you will use the train and validation subsets.  \n",
    "As you do this, you will adjust the number epochs, the learning rate, and other \"hyperparameters\" to reduce the train and validation set losses.  \n",
    "Once this process is complete, you will evaluate your model on a separate test set.  \n",
    "You can apply its .evaluate(x,y) method to compute the loss and metric values for features x and labels y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.evaluate(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models with the Estimators API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the Estimators API?\n",
    "- High level submodule\n",
    "- Less flexible\n",
    "- Enforces best practices\n",
    "- Faster deployment\n",
    "- Many premade models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model specification and training\n",
    "1. Define feature column\n",
    "2. Lad and transform data\n",
    "3. Define an estimator\n",
    "4. Apply train operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a numeric feature column\n",
    "size = tf.feature_column.numeric_column(\"size\")\n",
    "\n",
    "# Define a categorical feature column\n",
    "rooms = tf.feature_column.categorical_column_with_vocabulary_list(\"rooms\", [\"1\", \"2\", \"3\", \"4\", \"5\"])\n",
    "\n",
    "# Create feature column list\n",
    "features_list = [size, rooms]\n",
    "\n",
    "# Define a matrix feature column\n",
    "features_list = [tf.feature_column.numeric_column('image', shape(784,))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define input data function\n",
    "def input_fn():\n",
    "    # Define feature dictionary\n",
    "    features = {\"size\": [1340, 1690, 2720], \"rooms\": [1, 3, 4]}\n",
    "    # Define labels\n",
    "    labels = [221900, 538000, 180000]\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train a regression estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a deep neural network regression\n",
    "model0 = tf.estimator.DNNRegressor(feature_columns=feature_list, hidden_units=[10, 6, 6, 3])\n",
    "\n",
    "# Train the regression model\n",
    "model0.train(input_fn, steps=20)\n",
    "\n",
    "# Define a deep neural network classifier\n",
    "model1 = tf.estimator.DNNClassifier(feature_columns=feature_list, hidden_units=[32, 16, 8], n_classes=4)\n",
    "\n",
    "# Train the classifier\n",
    "model1.train(input_fn, steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow extensions\n",
    "- TensorFlow Hub\n",
    "    - Pretrained models\n",
    "    - Transfer learning\n",
    "- TensorFlow Probability\n",
    "    - More statistical distributions\n",
    "    - Trainable distributions\n",
    "    - Extended set of optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow 2.0\n",
    "- eager_execution()\n",
    "- Tighter keras integration\n",
    "- Estimators\n",
    "- function()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
